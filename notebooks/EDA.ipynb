{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple \n",
    "import shutil, time, re\n",
    "from src.config.spark_manager import spark_session\n",
    "from src.constants.training_pipeline import *\n",
    "from src.components.data_validation import add_mean_indicator_col_per_user\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.pipeline import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, MinMaxScaler, VectorAssembler\n",
    "from pyspark.sql.functions import lit, col, DataFrame, min, max\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_downloaded_data(paths:list):\n",
    "        try:\n",
    "            #logging.info(\"Entered read_downloaded_data method\")\n",
    "            for i, path in enumerate(paths):\n",
    "                file_list = os.listdir(path)\n",
    "                for j,file in enumerate(file_list):\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    user = file.split(sep='.')[0]\n",
    "                    user_type = re.split('/', path)[-1]\n",
    "                    temp_df = spark_session.read.csv(file_path, header=True, inferSchema=True)\n",
    "                    temp_df = temp_df.withColumn(USER_COLUMN_NAME, lit(f\"{user}_{user_type}\"))\n",
    "                    if j == 0:\n",
    "                        temp_df1 = temp_df\n",
    "                    else:\n",
    "                        temp_df1 = temp_df1.union(temp_df)\n",
    "                temp_df1 = temp_df1.withColumn(TARGET_COLUMN_NAME, lit(f\"{user_type}\"))\n",
    "                if i == 0:    \n",
    "                    temp_df2 = temp_df1\n",
    "                else:\n",
    "                    temp_df2 = temp_df2.union(temp_df1)\n",
    "            #logging.info(f\"reading of CSV is done\")\n",
    "            return temp_df2     \n",
    "        except Exception as e:\n",
    "            #logging.error(e)\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_downloaded_data(['../user_downloaded_data/UBE', '../user_downloaded_data/UGE/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134764"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for column in INDICATOR_COLS:\n",
    "    df = df.filter(col(column) < INDICATOR_THRESHOLD)\n",
    "df = add_mean_indicator_col_per_user(df, USER_COLUMN_NAME, INDICATOR_COLS)\n",
    "df = df.drop(*COLS_TO_BE_REMOVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126632, 19]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df.count(), len(df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(data: DataFrame, train_percentage:float, \n",
    "                                categorical_cols: list)-> DataFrame:\n",
    "        try:\n",
    "            #logging.info(\"Entered prepare_train_test_data method\")\n",
    "            train, test = data.randomSplit([train_percentage, 1 - train_percentage], seed=43)\n",
    "            #train, test = data.randomSplit([train_percentage, 1 - train_percentage])\n",
    "            empty_rdd = spark_session.sparkContext.emptyRDD()\n",
    "            temp_df_1 = spark_session.createDataFrame(empty_rdd, schema=train.schema)\n",
    "            for column in categorical_cols:\n",
    "                cat_train_df = train.select(col(column))\n",
    "                cat_test_df = test.select(col(column))\n",
    "                df_diff = cat_test_df.subtract(cat_train_df).collect()\n",
    "                print(f\"column {column} in test dataset has {len(df_diff)} values not present in train dataset\")\n",
    "                if len(df_diff) > 0:\n",
    "                    for row in df_diff:\n",
    "                        temp_df = test.where(col(column) == row[column]).dropDuplicates([column])\n",
    "                        temp_df_1 = temp_df_1.union(temp_df)\n",
    "            if temp_df_1.count() > 0:\n",
    "                train = train.union(temp_df_1)\n",
    "            #logging.info(f\"train and test split done. train count is {train.count()}, test count is {test.count()}\")\n",
    "            return train, test\n",
    "        except Exception as e:\n",
    "            #logging.error(e)\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark_session.read.csv('./final_data2.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=======================================================>(63 + 1) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|       day| user|count|\n",
      "+----------+-----+-----+\n",
      "|2021-06-16|152.0| 3803|\n",
      "|2021-06-12|151.0|  844|\n",
      "|2021-06-14|153.0| 1635|\n",
      "|2021-06-15|152.0| 3485|\n",
      "|2021-06-12|152.0| 3777|\n",
      "|2021-06-14|151.0| 1037|\n",
      "|2021-06-10|153.0|  220|\n",
      "|2021-06-14|150.0|  402|\n",
      "|2021-06-15|151.0| 1789|\n",
      "|2021-06-12|154.0| 6236|\n",
      "|2021-06-10|150.0|  165|\n",
      "|2021-06-12|153.0| 1132|\n",
      "|2021-06-12|150.0|  549|\n",
      "|2021-06-13|153.0| 1443|\n",
      "|2021-06-13|150.0|  573|\n",
      "|2021-06-13|152.0| 3856|\n",
      "|2021-06-14|154.0| 4100|\n",
      "|2021-06-16|153.0| 1209|\n",
      "|2021-06-10|151.0|  726|\n",
      "|2021-06-10|154.0|  210|\n",
      "+----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(*['day', 'user']).groupBy(['day', 'user']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===============>                                       (18 + 46) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+\n",
      "|       day|result| count|\n",
      "+----------+------+------+\n",
      "|2021-06-11|   uge|261331|\n",
      "|2021-06-12|   uge|294052|\n",
      "|2021-06-16|   uge|270988|\n",
      "|2021-06-13|   uge|303593|\n",
      "|2021-06-15|   uge|265336|\n",
      "|2021-06-10|   uge|117804|\n",
      "|2021-06-14|   uge|311565|\n",
      "|2021-06-12|   ube|294773|\n",
      "|2021-06-14|   ube|314681|\n",
      "|2021-06-11|   ube|277704|\n",
      "|2021-06-16|   ube|274946|\n",
      "|2021-06-15|   ube|270898|\n",
      "|2021-06-10|   ube|136994|\n",
      "|2021-06-13|   ube|292707|\n",
      "+----------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(*['day', 'result']).groupBy(['day', 'result']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================>   (60 + 4) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|  min(day)|  max(day)|\n",
      "+----------+----------+\n",
      "|2021-06-10|2021-06-16|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(min('day'), max('day')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = spark_session.read.csv('../user_downloaded_data/UBE/user1.csv', inferSchema=True, header=True)\n",
    "df_1 = df_1.withColumn('day', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vect = VectorAssembler(inputCols=['hour'], outputCol='v_hour')\n",
    "df_2 = vect.transform(df_1)\n",
    "min_max_scalar = MinMaxScaler(inputCol='v_hour', outputCol='s_hour')\n",
    "df_2 = min_max_scalar.fit(df_2).transform(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------------------+----------+----------+----------+----------+----------+----------+----------+----------+------+--------------------+\n",
      "|       day|hour|       specifictime|indicator1|indicator2|indicator3|indicator4|indicator5|indicator6|indicator7|indicator8|v_hour|              s_hour|\n",
      "+----------+----+-------------------+----------+----------+----------+----------+----------+----------+----------+----------+------+--------------------+\n",
      "|2021-06-10|  18|2021-06-10 18:09:59|      45.0|      36.0|       3.0|      45.0|     229.0|      45.0|       0.2|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:09:59|      33.0|       3.0|       0.0|      33.0|       2.0|      33.0|       0.0|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:09:59|      24.0|       2.0|       5.0|      24.0|       2.0|      24.0|       0.0|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:09:59|       5.0|       1.0|       2.0|      25.5|     300.0|      18.0|    0.3404|    0.1429|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:09:59|       5.0|       3.0|       5.0|       5.0|       2.0|       5.0|       0.0|      0.24|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:09:59|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|    0.4375|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:14:59|      13.0|     55.33|       3.0|      86.0|      50.0|      14.0|       0.0|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:14:59|      24.5|       2.0|      20.0|      24.5|       9.0|      25.0|    0.0714|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:14:59|      26.0|     158.0|       2.0|      25.0|       0.0|      25.0|       0.0|    0.0556|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:14:59|      9.33|       2.0|       2.0|      9.67|       8.0|      10.0|      0.12|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:14:59|       8.0|       3.0|       6.0|       9.0|      25.0|      13.0|    0.0667|    0.2308|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:19:59|      46.0|     250.0|       1.0|      46.0|      79.0|      50.0|       0.0|    0.2727|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:19:59|     46.05|    411.24|      5.19|     45.76|      15.0|      45.0|       0.0|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:19:59|       5.0|       1.0|       0.0|       5.0|       1.0|       5.0|       0.0|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:24:59|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|    0.4375|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:24:59|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.3|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:24:59|       5.0|       5.0|      41.0|       5.0|       9.0|       6.0|       0.0|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:24:59|      13.0|       2.0|      22.0|      13.0|       6.0|      13.0|       0.0|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:29:59|      11.0|       2.0|       1.0|      11.0|     238.0|      10.0|     0.125|       0.0|[18.0]|[0.7826086956521738]|\n",
      "|2021-06-10|  18|2021-06-10 18:29:59|      47.5|     614.5|       3.0|      48.0|     146.0|      48.0|       0.0|       0.0|[18.0]|[0.7826086956521738]|\n",
      "+----------+----+-------------------+----------+----------+----------+----------+----------+----------+----------+----------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,IntegerType,true),StructField(day,StringType,true),StructField(hour,IntegerType,true),StructField(specifictime,StringType,true),StructField(indicator1,DoubleType,true),StructField(indicator2,DoubleType,true),StructField(indicator3,DoubleType,true),StructField(indicator4,DoubleType,true),StructField(indicator5,DoubleType,true),StructField(indicator6,DoubleType,true),StructField(indicator7,DoubleType,true),StructField(indicator8,DoubleType,true),StructField(user,DoubleType,true),StructField(result,StringType,true)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
