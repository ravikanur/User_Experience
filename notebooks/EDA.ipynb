{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/08 08:50:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/08 08:50:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple \n",
    "import shutil, time, re\n",
    "from src.config.spark_manager import spark_session\n",
    "from src.constants.training_pipeline import *\n",
    "from src.components.data_validation import add_mean_indicator_col_per_user\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.pipeline import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.sql.functions import lit, col, DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_downloaded_data(paths:list):\n",
    "        try:\n",
    "            #logging.info(\"Entered read_downloaded_data method\")\n",
    "            for i, path in enumerate(paths):\n",
    "                file_list = os.listdir(path)\n",
    "                for j,file in enumerate(file_list):\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    user = file.split(sep='.')[0]\n",
    "                    user_type = re.split('/', path)[-1]\n",
    "                    temp_df = spark_session.read.csv(file_path, header=True, inferSchema=True)\n",
    "                    temp_df = temp_df.withColumn(USER_COLUMN_NAME, lit(f\"{user}_{user_type}\"))\n",
    "                    if j == 0:\n",
    "                        temp_df1 = temp_df\n",
    "                    else:\n",
    "                        temp_df1 = temp_df1.union(temp_df)\n",
    "                temp_df1 = temp_df1.withColumn(TARGET_COLUMN_NAME, lit(f\"{user_type}\"))\n",
    "                if i == 0:    \n",
    "                    temp_df2 = temp_df1\n",
    "                else:\n",
    "                    temp_df2 = temp_df2.union(temp_df1)\n",
    "            #logging.info(f\"reading of CSV is done\")\n",
    "            return temp_df2     \n",
    "        except Exception as e:\n",
    "            #logging.error(e)\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_downloaded_data(['../user_downloaded_data/UBE', '../user_downloaded_data/UGE/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134764"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for column in INDICATOR_COLS:\n",
    "    df = df.filter(col(column) < INDICATOR_THRESHOLD)\n",
    "df = add_mean_indicator_col_per_user(df, USER_COLUMN_NAME, INDICATOR_COLS)\n",
    "df = df.drop(*COLS_TO_BE_REMOVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126632, 19]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df.count(), len(df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(data: DataFrame, train_percentage:float, \n",
    "                                categorical_cols: list)-> DataFrame:\n",
    "        try:\n",
    "            #logging.info(\"Entered prepare_train_test_data method\")\n",
    "            train, test = data.randomSplit([train_percentage, 1 - train_percentage], seed=43)\n",
    "            #train, test = data.randomSplit([train_percentage, 1 - train_percentage])\n",
    "            empty_rdd = spark_session.sparkContext.emptyRDD()\n",
    "            temp_df_1 = spark_session.createDataFrame(empty_rdd, schema=train.schema)\n",
    "            for column in categorical_cols:\n",
    "                cat_train_df = train.select(col(column))\n",
    "                cat_test_df = test.select(col(column))\n",
    "                df_diff = cat_test_df.subtract(cat_train_df).collect()\n",
    "                print(f\"column {column} in test dataset has {len(df_diff)} values not present in train dataset\")\n",
    "                if len(df_diff) > 0:\n",
    "                    for row in df_diff:\n",
    "                        temp_df = test.where(col(column) == row[column]).dropDuplicates([column])\n",
    "                        temp_df_1 = temp_df_1.union(temp_df)\n",
    "            if temp_df_1.count() > 0:\n",
    "                train = train.union(temp_df_1)\n",
    "            #logging.info(f\"train and test split done. train count is {train.count()}, test count is {test.count()}\")\n",
    "            return train, test\n",
    "        except Exception as e:\n",
    "            #logging.error(e)\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark_session.read.csv('./final_data2.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=======================================================>(63 + 1) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|       day| user|count|\n",
      "+----------+-----+-----+\n",
      "|2021-06-16|152.0| 3803|\n",
      "|2021-06-12|151.0|  844|\n",
      "|2021-06-14|153.0| 1635|\n",
      "|2021-06-15|152.0| 3485|\n",
      "|2021-06-12|152.0| 3777|\n",
      "|2021-06-14|151.0| 1037|\n",
      "|2021-06-10|153.0|  220|\n",
      "|2021-06-14|150.0|  402|\n",
      "|2021-06-15|151.0| 1789|\n",
      "|2021-06-12|154.0| 6236|\n",
      "|2021-06-10|150.0|  165|\n",
      "|2021-06-12|153.0| 1132|\n",
      "|2021-06-12|150.0|  549|\n",
      "|2021-06-13|153.0| 1443|\n",
      "|2021-06-13|150.0|  573|\n",
      "|2021-06-13|152.0| 3856|\n",
      "|2021-06-14|154.0| 4100|\n",
      "|2021-06-16|153.0| 1209|\n",
      "|2021-06-10|151.0|  726|\n",
      "|2021-06-10|154.0|  210|\n",
      "+----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(*['day', 'user']).groupBy(['day', 'user']).count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
