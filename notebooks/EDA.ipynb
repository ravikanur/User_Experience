{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/26 14:53:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/26 14:53:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple \n",
    "import shutil, time, re\n",
    "from src.config.spark_manager import spark_session\n",
    "from src.constants.training_pipeline import *\n",
    "from src.components.data_validation import add_mean_indicator_col_per_user\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.pipeline import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.sql.functions import lit, col, DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_downloaded_data(paths:list):\n",
    "        try:\n",
    "            #logging.info(\"Entered read_downloaded_data method\")\n",
    "            for i, path in enumerate(paths):\n",
    "                file_list = os.listdir(path)\n",
    "                for j,file in enumerate(file_list):\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    user = file.split(sep='.')[0]\n",
    "                    user_type = re.split('/', path)[-1]\n",
    "                    temp_df = spark_session.read.csv(file_path, header=True, inferSchema=True)\n",
    "                    temp_df = temp_df.withColumn(USER_COLUMN_NAME, lit(f\"{user}_{user_type}\"))\n",
    "                    if j == 0:\n",
    "                        temp_df1 = temp_df\n",
    "                    else:\n",
    "                        temp_df1 = temp_df1.union(temp_df)\n",
    "                temp_df1 = temp_df1.withColumn(TARGET_COLUMN_NAME, lit(f\"{user_type}\"))\n",
    "                if i == 0:    \n",
    "                    temp_df2 = temp_df1\n",
    "                else:\n",
    "                    temp_df2 = temp_df2.union(temp_df1)\n",
    "            #logging.info(f\"reading of CSV is done\")\n",
    "            return temp_df2     \n",
    "        except Exception as e:\n",
    "            #logging.error(e)\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_downloaded_data(['../user_downloaded_data/UBE', '../user_downloaded_data/UGE/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134764"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for column in INDICATOR_COLS:\n",
    "    df = df.filter(col(column) < INDICATOR_THRESHOLD)\n",
    "df = add_mean_indicator_col_per_user(df, USER_COLUMN_NAME, INDICATOR_COLS)\n",
    "df = df.drop(*COLS_TO_BE_REMOVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126632, 19]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df.count(), len(df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(data: DataFrame, train_percentage:float, \n",
    "                                categorical_cols: list)-> DataFrame:\n",
    "        try:\n",
    "            #logging.info(\"Entered prepare_train_test_data method\")\n",
    "            train, test = data.randomSplit([train_percentage, 1 - train_percentage], seed=42)\n",
    "            #train, test = data.randomSplit([train_percentage, 1 - train_percentage])\n",
    "            empty_rdd = spark_session.sparkContext.emptyRDD()\n",
    "            temp_df_1 = spark_session.createDataFrame(empty_rdd, schema=train.schema)\n",
    "            for column in categorical_cols:\n",
    "                cat_train_df = train.select(col(column))\n",
    "                cat_test_df = test.select(col(column))\n",
    "                df_diff = cat_test_df.subtract(cat_train_df).collect()\n",
    "                print(f\"column {column} in test dataset has {len(df_diff)} values not present in train dataset\")\n",
    "                if len(df_diff) > 0:\n",
    "                    for row in df_diff:\n",
    "                        temp_df = test.where(col(column) == row[column]).dropDuplicates([column])\n",
    "                        temp_df_1 = temp_df_1.union(temp_df)\n",
    "            if temp_df_1.count() > 0:\n",
    "                train = train.union(temp_df_1)\n",
    "            #logging.info(f\"train and test split done. train count is {train.count()}, test count is {test.count()}\")\n",
    "            return train, test\n",
    "        except Exception as e:\n",
    "            #logging.error(e)\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "str_indexer = StringIndexer(inputCol=TARGET_COLUMN_NAME, outputCol=ENCODED_TARGET_COL_NAME)\n",
    "df = str_indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o553.collectToPython.\n",
      ": java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train, test \u001b[38;5;241m=\u001b[39m prepare_train_test_data(df, \u001b[38;5;241m0.7\u001b[39m, LABEL_FEATURES \u001b[38;5;241m+\u001b[39m [TARGET_COLUMN_NAME])\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "train, test = prepare_train_test_data(df, 0.7, LABEL_FEATURES + [TARGET_COLUMN_NAME])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
